{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c7ce3a-d2ba-4a1c-946a-dc8383aa87af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Class 'Chaetoceros_didymus_flagellate' has 1427 images, reducing to 800\n",
      "   ‚úÖ Deleted 627 extra images, kept 800\n",
      "‚ö†Ô∏è Class 'Chaetoceros_pennate' has 970 images, reducing to 800\n",
      "   ‚úÖ Deleted 170 extra images, kept 800\n",
      "‚ö†Ô∏è Class 'G_delicatula_detritus' has 816 images, reducing to 800\n",
      "   ‚úÖ Deleted 16 extra images, kept 800\n",
      "‚ö†Ô∏è Class 'flagellate_sp3' has 1905 images, reducing to 800\n",
      "   ‚úÖ Deleted 1105 extra images, kept 800\n",
      "‚ö†Ô∏è Class 'pennates_on_diatoms' has 851 images, reducing to 800\n",
      "   ‚úÖ Deleted 51 extra images, kept 800\n",
      "‚úÖ Cleanup complete! (Small classes kept intact)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATASET_DIR = r\"D:\\Marine Data\"  # Your dataset root folder\n",
    "MAX_IMAGES = 800                 # Cap classes to 600 images (adjust if needed)\n",
    "\n",
    "# ==== STEP 1: Iterate through class folders ====\n",
    "classes = sorted(os.listdir(DATASET_DIR))\n",
    "\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(DATASET_DIR, cls)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "\n",
    "    # Collect all image files in this class folder\n",
    "    images = [os.path.join(cls_path, f) for f in os.listdir(cls_path)\n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    count = len(images)\n",
    "\n",
    "    # ---- Only reduce if class has more than MAX_IMAGES ----\n",
    "    if count > MAX_IMAGES:\n",
    "        print(f\"‚ö†Ô∏è Class '{cls}' has {count} images, reducing to {MAX_IMAGES}\")\n",
    "        # Randomly pick images to KEEP\n",
    "        keep_images = set(random.sample(images, MAX_IMAGES))\n",
    "        deleted = 0\n",
    "        for img_path in images:\n",
    "            if img_path not in keep_images:\n",
    "                try:\n",
    "                    os.remove(img_path)\n",
    "                    deleted += 1\n",
    "                except:\n",
    "                    pass\n",
    "        print(f\"   ‚úÖ Deleted {deleted} extra images, kept {MAX_IMAGES}\")\n",
    "\n",
    "print(\"‚úÖ Cleanup complete! (Small classes kept intact)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e39a17fd-0579-4582-b251-4c3337c11324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¢ Found 15 small classes (< 70 images):\n",
      "\n",
      "  - Akashiwo: 5 images\n",
      "  - Bacillaria: 13 images\n",
      "  - Bidulphia: 31 images\n",
      "  - Cochlodinium: 14 images\n",
      "  - Didinium_sp: 24 images\n",
      "  - Euplotes_sp: 25 images\n",
      "  - Hemiaulus: 18 images\n",
      "  - Karenia: 4 images\n",
      "  - Stephanopyxis: 57 images\n",
      "  - Strombidium_capitatum: 47 images\n",
      "  - Tiarina_fusus: 13 images\n",
      "  - Tontonia_appendiculariformis: 46 images\n",
      "  - bubble: 19 images\n",
      "  - pollen: 19 images\n",
      "  - zooplankton: 37 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATASET_DIR = r\"D:\\Marine Data\"  # Your dataset root folder\n",
    "MIN_IMAGES = 70                 # Threshold for \"small\" classes\n",
    "\n",
    "# ==== STEP 1: Scan all class folders ====\n",
    "classes = sorted(os.listdir(DATASET_DIR))\n",
    "small_classes = []\n",
    "\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(DATASET_DIR, cls)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "\n",
    "    # Count images\n",
    "    images = [f for f in os.listdir(cls_path)\n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    count = len(images)\n",
    "\n",
    "    if count < MIN_IMAGES:\n",
    "        small_classes.append((cls, count))\n",
    "\n",
    "# ==== STEP 2: Display results ====\n",
    "if small_classes:\n",
    "    print(f\"üì¢ Found {len(small_classes)} small classes (< {MIN_IMAGES} images):\\n\")\n",
    "    for cls, count in small_classes:\n",
    "        print(f\"  - {cls}: {count} images\")\n",
    "else:\n",
    "    print(f\"‚úÖ No classes have fewer than {MIN_IMAGES} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747c5977-cfd4-4a4e-9ba0-749ac9dd8236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 87 classes:\n",
      "\n",
      "001. Amphidinium_sp\n",
      "002. Asterionellopsis\n",
      "003. Cerataulina\n",
      "004. Cerataulina_flagellate\n",
      "005. Ceratium\n",
      "006. Chaetoceros\n",
      "007. Chaetoceros_didymus\n",
      "008. Chaetoceros_didymus_flagellate\n",
      "009. Chaetoceros_flagellate\n",
      "010. Chaetoceros_other\n",
      "011. Chaetoceros_pennate\n",
      "012. Chrysochromulina\n",
      "013. Ciliate_mix\n",
      "014. Cochlodinium\n",
      "015. Corethron\n",
      "016. Coscinodiscus\n",
      "017. Cylindrotheca\n",
      "018. DactFragCerataul\n",
      "019. Dactyliosolen\n",
      "020. Delphineis\n",
      "021. Dictyocha\n",
      "022. Dinobryon\n",
      "023. Dinophysis\n",
      "024. Ditylum\n",
      "025. Ditylum_parasite\n",
      "026. Emiliania_huxleyi\n",
      "027. Ephemera\n",
      "028. Eucampia\n",
      "029. Euglena\n",
      "030. G_delicatula_detritus\n",
      "031. G_delicatula_external_parasite\n",
      "032. G_delicatula_parasite\n",
      "033. Gonyaulax\n",
      "034. Guinardia_delicatula\n",
      "035. Guinardia_flaccida\n",
      "036. Guinardia_striata\n",
      "037. Gyrodinium\n",
      "038. Heterocapsa_triquetra\n",
      "039. Katodinium_or_Torodinium\n",
      "040. Laboea_strobila\n",
      "041. Lauderia\n",
      "042. Leegaardiella_ovalis\n",
      "043. Leptocylindrus\n",
      "044. Leptocylindrus_mediterraneus\n",
      "045. Licmophora\n",
      "046. Mesodinium_sp\n",
      "047. Odontella\n",
      "048. Paralia\n",
      "049. Parvicorbicula_socialis\n",
      "050. Phaeocystis\n",
      "051. Pleuronema_sp\n",
      "052. Pleurosigma\n",
      "053. Prorocentrum\n",
      "054. Proterythropsis_sp\n",
      "055. Protoperidinium\n",
      "056. Pseudochattonella_farcimen\n",
      "057. Pseudonitzschia\n",
      "058. Pyramimonas_longicauda\n",
      "059. Rhizosolenia\n",
      "060. Skeletonema\n",
      "061. Strobilidium_morphotype1\n",
      "062. Strombidium_conicum\n",
      "063. Strombidium_inclinatum\n",
      "064. Strombidium_morphotype1\n",
      "065. Strombidium_morphotype2\n",
      "066. Strombidium_oculatum\n",
      "067. Strombidium_wulffi\n",
      "068. Thalassionema\n",
      "069. Thalassiosira\n",
      "070. Thalassiosira_dirty\n",
      "071. Tintinnid\n",
      "072. Tontonia_gracillima\n",
      "073. amoeba\n",
      "074. bead\n",
      "075. clusterflagellate\n",
      "076. detritus\n",
      "077. diatom_flagellate\n",
      "078. dino30\n",
      "079. dino_large1\n",
      "080. flagellate_sp3\n",
      "081. kiteflagellates\n",
      "082. mixotroph\n",
      "083. mixotroph_elongated\n",
      "084. pennate\n",
      "085. pennate_morphotype1\n",
      "086. pennates_on_diatoms\n",
      "087. spore\n",
      "\n",
      "üìÑ Labels saved to 'class_labels.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATASET_DIR = r\"D:\\Marine Data\"  # Your dataset root folder\n",
    "\n",
    "# ==== STEP 1: Scan for class folders ====\n",
    "classes = sorted([d for d in os.listdir(DATASET_DIR)\n",
    "                  if os.path.isdir(os.path.join(DATASET_DIR, d))])\n",
    "\n",
    "print(f\"‚úÖ Found {len(classes)} classes:\\n\")\n",
    "for i, cls in enumerate(classes, 1):\n",
    "    print(f\"{i:03d}. {cls}\")\n",
    "\n",
    "# ==== OPTIONAL: Save to a text file ====\n",
    "output_file = \"class_labels.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for cls in classes:\n",
    "        f.write(f\"{cls}\\n\")\n",
    "\n",
    "print(f\"\\nüìÑ Labels saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b81889f2-752d-4a2e-bf97-a50664e8c8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 87 classes.\n",
      "\n",
      "Amphidinium_sp: 210 images\n",
      "Asterionellopsis: 600 images\n",
      "Cerataulina: 600 images\n",
      "Cerataulina_flagellate: 142 images\n",
      "Ceratium: 784 images\n",
      "Chaetoceros: 600 images\n",
      "Chaetoceros_didymus: 502 images\n",
      "Chaetoceros_didymus_flagellate: 800 images\n",
      "Chaetoceros_flagellate: 149 images\n",
      "Chaetoceros_other: 276 images\n",
      "Chaetoceros_pennate: 800 images\n",
      "Chrysochromulina: 515 images\n",
      "Ciliate_mix: 600 images\n",
      "Cochlodinium: 14 images\n",
      "Corethron: 600 images\n",
      "Coscinodiscus: 711 images\n",
      "Cylindrotheca: 600 images\n",
      "DactFragCerataul: 600 images\n",
      "Dactyliosolen: 600 images\n",
      "Delphineis: 401 images\n",
      "Dictyocha: 600 images\n",
      "Dinobryon: 600 images\n",
      "Dinophysis: 357 images\n",
      "Ditylum: 600 images\n",
      "Ditylum_parasite: 258 images\n",
      "Emiliania_huxleyi: 148 images\n",
      "Ephemera: 633 images\n",
      "Eucampia: 600 images\n",
      "Euglena: 764 images\n",
      "G_delicatula_detritus: 800 images\n",
      "G_delicatula_external_parasite: 433 images\n",
      "G_delicatula_parasite: 600 images\n",
      "Gonyaulax: 564 images\n",
      "Guinardia_delicatula: 600 images\n",
      "Guinardia_flaccida: 600 images\n",
      "Guinardia_striata: 600 images\n",
      "Gyrodinium: 677 images\n",
      "Heterocapsa_triquetra: 600 images\n",
      "Katodinium_or_Torodinium: 395 images\n",
      "Laboea_strobila: 600 images\n",
      "Lauderia: 295 images\n",
      "Leegaardiella_ovalis: 245 images\n",
      "Leptocylindrus: 600 images\n",
      "Leptocylindrus_mediterraneus: 392 images\n",
      "Licmophora: 313 images\n",
      "Mesodinium_sp: 600 images\n",
      "Odontella: 86 images\n",
      "Paralia: 666 images\n",
      "Parvicorbicula_socialis: 78 images\n",
      "Phaeocystis: 600 images\n",
      "Pleuronema_sp: 115 images\n",
      "Pleurosigma: 600 images\n",
      "Prorocentrum: 600 images\n",
      "Proterythropsis_sp: 665 images\n",
      "Protoperidinium: 78 images\n",
      "Pseudochattonella_farcimen: 283 images\n",
      "Pseudonitzschia: 600 images\n",
      "Pyramimonas_longicauda: 616 images\n",
      "Rhizosolenia: 600 images\n",
      "Skeletonema: 600 images\n",
      "Strobilidium_morphotype1: 250 images\n",
      "Strombidium_conicum: 87 images\n",
      "Strombidium_inclinatum: 222 images\n",
      "Strombidium_morphotype1: 726 images\n",
      "Strombidium_morphotype2: 311 images\n",
      "Strombidium_oculatum: 152 images\n",
      "Strombidium_wulffi: 157 images\n",
      "Thalassionema: 600 images\n",
      "Thalassiosira: 600 images\n",
      "Thalassiosira_dirty: 600 images\n",
      "Tintinnid: 600 images\n",
      "Tontonia_gracillima: 361 images\n",
      "amoeba: 219 images\n",
      "bead: 395 images\n",
      "clusterflagellate: 290 images\n",
      "detritus: 600 images\n",
      "diatom_flagellate: 728 images\n",
      "dino30: 600 images\n",
      "dino_large1: 180 images\n",
      "flagellate_sp3: 800 images\n",
      "kiteflagellates: 525 images\n",
      "mixotroph: 600 images\n",
      "mixotroph_elongated: 600 images\n",
      "pennate: 600 images\n",
      "pennate_morphotype1: 238 images\n",
      "pennates_on_diatoms: 800 images\n",
      "spore: 381 images\n",
      "\n",
      "üì∏ TOTAL IMAGES: 41987\n",
      "üìÑ Class distribution saved to 'class_distribution.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATASET_DIR = r\"D:\\Marine Data\"  # Your dataset root folder (after filtering)\n",
    "\n",
    "classes = sorted([d for d in os.listdir(DATASET_DIR)\n",
    "                  if os.path.isdir(os.path.join(DATASET_DIR, d))])\n",
    "\n",
    "print(f\"‚úÖ Found {len(classes)} classes.\\n\")\n",
    "\n",
    "class_counts = []\n",
    "total_images = 0  # keep track of total count\n",
    "\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(DATASET_DIR, cls)\n",
    "    images = [f for f in os.listdir(cls_path)\n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    count = len(images)\n",
    "    total_images += count\n",
    "    class_counts.append((cls, count))\n",
    "\n",
    "# ==== Print summary ====\n",
    "for cls, count in class_counts:\n",
    "    print(f\"{cls}: {count} images\")\n",
    "\n",
    "print(\"\\nüì∏ TOTAL IMAGES:\", total_images)\n",
    "\n",
    "# ==== OPTIONAL: Save to CSV ====\n",
    "with open(\"class_distribution.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Class\", \"Image_Count\"])\n",
    "    writer.writerows(class_counts)\n",
    "\n",
    "print(\"üìÑ Class distribution saved to 'class_distribution.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab292d8b-b67c-4f3c-ba52-487892809184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkr30\\Image Segmentation_Plant Disease\\plant_disease_cls\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383bbe8a-baff-4182-8132-4932695b474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current Working Directory: C:\\Users\\vkr30\\OneDrive\\Documents\\Jupyter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ==== Change working directory ====\n",
    "new_dir = r\"C:\\Users\\vkr30\\OneDrive\\Documents\\Jupyter\"  # <-- Change this to your desired folder\n",
    "os.chdir(new_dir)\n",
    "\n",
    "# Verify the change\n",
    "print(\"üìÇ Current Working Directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe812ad2-d18a-45d4-b321-66e5eb11d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 87 class names to C:\\Users\\vkr30\\OneDrive\\Documents\\classes.txt\n",
      "üîç First 5 classes: ['Amphidinium_sp', 'Asterionellopsis', 'Cerataulina', 'Cerataulina_flagellate', 'Ceratium'] ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "DATASET_PATH = r\"D:\\Marine Data\"  # Your full dataset root path\n",
    "OUTPUT_FILE = r\"C:\\Users\\vkr30\\OneDrive\\Documents\\classes.txt\"  # Where to save the file\n",
    "\n",
    "# === GENERATE CLASS NAMES ===\n",
    "class_names = sorted([d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for cls in class_names:\n",
    "        f.write(f\"{cls}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Saved {len(class_names)} class names to {OUTPUT_FILE}\")\n",
    "print(\"üîç First 5 classes:\", class_names[:5], \"...\") if len(class_names) > 5 else print(\"üîç All classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2f034b-d5e4-4f34-ba8f-9b89922d2034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 87 class names from file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkr30\\AppData\\Local\\Temp\\ipykernel_4004\\3932292581.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_PATH, map_location=\"cpu\")\n"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'quantized::linear_dynamic' to ONNX opset version 17 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnsupportedOperatorError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m dummy_input = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# === 5Ô∏è‚É£ EXPORT TO ONNX ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantized_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mONNX_OUTPUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m17\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Exported quantized model to ONNX: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mONNX_OUTPUT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\test_final\\Lib\\site-packages\\torch\\onnx\\__init__.py:375\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, report, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining, **_)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe exporter only supports dynamic shapes \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    372\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    373\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\test_final\\Lib\\site-packages\\torch\\onnx\\utils.py:502\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    500\u001b[39m     args = args + (kwargs,)\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\test_final\\Lib\\site-packages\\torch\\onnx\\utils.py:1564\u001b[39m, in \u001b[36m_export\u001b[39m\u001b[34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m   1561\u001b[39m     dynamic_axes = {}\n\u001b[32m   1562\u001b[39m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[32m-> \u001b[39m\u001b[32m1564\u001b[39m graph, params_dict, torch_out = \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1577\u001b[39m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[32m   1578\u001b[39m defer_weight_export = (\n\u001b[32m   1579\u001b[39m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states.ExportTypes.PROTOBUF_FILE\n\u001b[32m   1580\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\test_final\\Lib\\site-packages\\torch\\onnx\\utils.py:1117\u001b[39m, in \u001b[36m_model_to_graph\u001b[39m\u001b[34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[39m\n\u001b[32m   1114\u001b[39m params_dict = _get_named_param_dict(graph, params)\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     graph = \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1128\u001b[39m     torch.onnx.log(\u001b[33m\"\u001b[39m\u001b[33mTorch IR graph at exception: \u001b[39m\u001b[33m\"\u001b[39m, graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\test_final\\Lib\\site-packages\\torch\\onnx\\utils.py:639\u001b[39m, in \u001b[36m_optimize_graph\u001b[39m\u001b[34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[39m\n\u001b[32m    636\u001b[39m     _C._jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[32m    637\u001b[39m _C._jit_pass_onnx_lint(graph)\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m graph = \u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m _C._jit_pass_onnx_lint(graph)\n\u001b[32m    641\u001b[39m _C._jit_pass_lint(graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\test_final\\Lib\\site-packages\\torch\\onnx\\utils.py:1848\u001b[39m, in \u001b[36m_run_symbolic_function\u001b[39m\u001b[34m(graph, block, node, inputs, env, values_in_env, new_nodes, operator_export_type)\u001b[39m\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m namespace == \u001b[33m\"\u001b[39m\u001b[33monnx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1843\u001b[39m         \u001b[38;5;66;03m# Clone node to trigger ONNX shape inference\u001b[39;00m\n\u001b[32m   1844\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m graph_context.op(\n\u001b[32m   1845\u001b[39m             op_name, *inputs, **attrs, outputs=node.outputsSize()\n\u001b[32m   1846\u001b[39m         )  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors.UnsupportedOperatorError(\n\u001b[32m   1849\u001b[39m         symbolic_function_name,\n\u001b[32m   1850\u001b[39m         opset_version,\n\u001b[32m   1851\u001b[39m         symbolic_function_group.get_min_supported()\n\u001b[32m   1852\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m symbolic_function_group\n\u001b[32m   1853\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1854\u001b[39m     )\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m   1857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m operator_export_type == _C_onnx.OperatorExportTypes.ONNX_FALLTHROUGH:\n",
      "\u001b[31mUnsupportedOperatorError\u001b[39m: Exporting the operator 'quantized::linear_dynamic' to ONNX opset version 17 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# === 1Ô∏è‚É£ CONFIG ===\n",
    "MODEL_PATH = r\"C:\\Users\\vkr30\\OneDrive\\Documents\\mobilenetv3_kd_int8.pth\"\n",
    "CLASSES_FILE = r\"C:\\Users\\vkr30\\OneDrive\\Documents\\marine_classes.txt\"\n",
    "ONNX_OUTPUT = r\"C:\\Users\\vkr30\\OneDrive\\Documents\\mobilenetv3_kd_int8.onnx\"\n",
    "\n",
    "# === 2Ô∏è‚É£ LOAD CLASS NAMES (to get num_classes) ===\n",
    "with open(CLASSES_FILE, \"r\") as f:\n",
    "    class_names = [line.strip() for line in f if line.strip()]\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(f\"‚úÖ Loaded {NUM_CLASSES} class names from file.\")\n",
    "\n",
    "# === 3Ô∏è‚É£ BUILD QUANTIZED MODEL ===\n",
    "float_model = models.mobilenet_v3_large(weights=None)\n",
    "float_model.classifier[3] = nn.Linear(float_model.classifier[3].in_features, NUM_CLASSES)\n",
    "\n",
    "# Apply dynamic quantization BEFORE loading weights\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    float_model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Load INT8 state_dict\n",
    "state_dict = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "quantized_model.load_state_dict(state_dict)\n",
    "quantized_model.eval()\n",
    "\n",
    "# === 4Ô∏è‚É£ DUMMY INPUT ===\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# === 5Ô∏è‚É£ EXPORT TO ONNX ===\n",
    "torch.onnx.export(\n",
    "    quantized_model,\n",
    "    dummy_input,\n",
    "    ONNX_OUTPUT,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "    opset_version=17\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Exported quantized model to ONNX: {ONNX_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbec34c-651a-44ee-ab03-59cf7e415d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
